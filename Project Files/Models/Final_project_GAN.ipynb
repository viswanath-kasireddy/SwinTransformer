{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18824,"status":"ok","timestamp":1702014636652,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"m3SUmYdeDiuO","outputId":"e99614de-5545-42a4-ec5f-ba4e76ad7109"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcIzCREyDQtg"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import os\n","import time\n","\n","import imageio\n","import glob\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense\n","#from tensorflow_docs.vis import embed\n","from sklearn.model_selection import train_test_split\n","#!pip install -q git+https://github.com/tensorflow/docs\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","import imageio\n","from scipy.linalg import sqrtm\n","from keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n","from keras.models import Model\n","from keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from PIL import Image\n","\n","\n","# Initialize rng\n","rng = np.random.default_rng(2022)\n","\n","auc = tf.keras.metrics.AUC()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51636,"status":"ok","timestamp":1702014719212,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"eEgK67hlDlLz","outputId":"48d6539e-d54f-49b6-e01a-b4debe645c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 16000 files belonging to 4 classes.\n","Found 4000 files belonging to 4 classes.\n","Found 4000 files belonging to 4 classes.\n"]}],"source":["batch_size = 32 # This is a tunable hyperparameter\n","shape = (128, 128) # note we are reducing the size of the image\n","\n","data_dir = '/content/drive/MyDrive/Eye_Dataset/'\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'train_new'),\n","                                                       seed=rng.integers(500000),\n","                                                       image_size=shape,\n","                                                       label_mode=\"categorical\",\n","                                                       color_mode='grayscale',\n","                                                       batch_size=batch_size)\n","val_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'validation/validation'),\n","                                                     seed=rng.integers(500000),\n","                                                     image_size=shape,\n","                                                     label_mode=\"categorical\",\n","                                                     color_mode='grayscale',\n","                                                     batch_size=batch_size)\n","test_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'test_new'),\n","                                                      seed=rng.integers(500000),\n","                                                      image_size=shape,\n","                                                      color_mode='grayscale',\n","                                                      batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76013,"status":"ok","timestamp":1702014795222,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"ScjjliJWD4KN","outputId":"34fcf218-2bb4-4137-ae92-e9dc50d4522e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 128, 128, 1)\n","(32, 4)\n"]}],"source":["for x, y in train_ds.take(1):\n","  print(x.shape)\n","  print(y.shape)"]},{"cell_type":"code","source":["# for images, labels in train_ds.take(1):\n","#     print(\"Images batch shape:\", images.shape)\n","#     print(\"Labels batch shape:\", labels.shape)"],"metadata":{"id":"ru4dJgF85M8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = '/content/drive/MyDrive/Eye_Dataset/train_new'"],"metadata":{"id":"Xo0ZETGIsJFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names_list = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n","class_names_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQZDq407j3Jd","executionInfo":{"status":"ok","timestamp":1702009579462,"user_tz":360,"elapsed":17,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"}},"outputId":"72f4ad04-babc-4266-9d9a-a3a4dec76535"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['DME', 'DRUSEN', 'NORMAL', 'CNV']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Number of classes (4 in your case: 'CNV', 'DME', 'DRUSEN', 'NORMAL')\n","num_classes = len(class_names_list)\n","num_classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffWPB55algL2","executionInfo":{"status":"ok","timestamp":1702009579462,"user_tz":360,"elapsed":15,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"}},"outputId":"81cdf62b-f948-466b-8942-4b0bdce0a0f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nk8odh9bcxQ4"},"outputs":[],"source":["buffer_size = 16000\n","batch_size = 32\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSi6Sghmc4xR"},"outputs":[],"source":["# normalize image pixels into range [-1, 1]\n","def normalize(image, label):\n","  \"\"\"\n","  The normalization function will transform each image pixel value to the range [-1, 1], while the labels remain unchanged.\n","\n","  \"\"\"\n","  image = (image - 127.5) / 127.5\n","  return image, label\n","\n","train_ds = train_ds.map(normalize)\n","val_ds = val_ds.map(normalize)\n","test_ds = test_ds.map(normalize)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2h6ZYibc459"},"outputs":[],"source":["# Batch and shuffle the data\n","train_ds = train_ds.shuffle(buffer_size)\n","\n","#val_ds = val_ds.batch(batch_size)\n","#test_ds = test_ds.batch(batch_size)\n"]},{"cell_type":"markdown","metadata":{"id":"uM4omI7_fuza"},"source":["Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_PQCgckc4om"},"outputs":[],"source":["def make_generator_model(num_classes):\n","    model = tf.keras.Sequential()\n","    model.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(100+num_classes,),\n","                           kernel_initializer='he_normal'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Reshape((8, 8, 1024)))\n","\n","    # Upsampling to 16x16\n","    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False,\n","                                     kernel_initializer='he_normal'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Upsampling to 32 x 32\n","    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False,\n","                                     kernel_initializer='he_normal'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Upsampling to 64 X 64\n","    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False,\n","                                     kernel_initializer='he_normal'))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Upsampling to 128 X 128\n","    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh',\n","                                     kernel_initializer='he_normal'))\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ITvYLsfEj8CQ"},"source":["Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"raKxFVUujqIO"},"outputs":[],"source":["def make_discriminator_model(num_classes):\n","    model = tf.keras.Sequential()\n","    model.add(layers.Input(shape=(128, 128, num_classes + 1)))\n","    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n","                            kernel_initializer='he_normal'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same',\n","                            kernel_initializer='he_normal'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same',\n","                            kernel_initializer='he_normal'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(4, kernel_initializer='he_normal'))\n","\n","    return model\n"]},{"cell_type":"code","source":["# Create a batch of one-hot encoded labels\n","# For simplicity, let's generate random labels. You might want to use specific labels depending on your application.\n","labels = tf.one_hot(tf.random.uniform([batch_size], minval=0, maxval=num_classes, dtype=tf.int32), depth=num_classes)\n","\n","# Measure training time\n","start_time = time.time()\n","\n","# Prepare labels for concatenation with the images (same as the previous example)\n","def concatenate_labels(images, labels):\n","    # Expand the labels to match the image dimensions\n","    labels_expanded = tf.expand_dims(tf.expand_dims(labels, 1), 2)\n","    # Tile to match batch, height, and width. No tiling needed for channels.\n","    labels_expanded = tf.tile(labels_expanded, [1, images.shape[1], images.shape[2], 1])\n","\n","    # Concatenate the labels with the images\n","    images_with_labels = tf.concat([images, labels_expanded], axis=-1)\n","    return images_with_labels\n","\n","end_time = time.time()\n","total_time = (end_time - start_time)/60\n","print(f\"Training time: {total_time} mins\")"],"metadata":{"id":"UNZG9OO6woih","executionInfo":{"status":"ok","timestamp":1702011122964,"user_tz":360,"elapsed":349,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aeaea4ad-89de-4e52-f301-19140ee47dd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training time: 2.5828679402669272e-06 mins\n"]}]},{"cell_type":"code","source":["# Parameters (modify these as per your requirements)\n","latent_dim = 100  # Size of the latent noise vector\n","num_classes = 4   # Number of classes for conditional GAN\n","batch_size = 32   # Batch size for training\n","num_epochs = 10   # Number of epochs for training\n","learning_rate = 0.0002  # Learning rate for optimizers\n","\n","# Initialize the generator and discriminator\n","generator = make_generator_model(num_classes)\n","discriminator = make_discriminator_model(num_classes)\n","\n","# Initialize optimizer with gradient clipping\n","g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n","d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n","\n","# Loss function\n","loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","# Measure training time\n","start_time = time.time()\n","print(\"start \", start_time)\n","# Training loop\n","for epoch in range(num_epochs):\n","    print(f\"Starting Epoch {epoch}\")\n","    for batch, (real_images, labels) in enumerate(train_ds):\n","        print(f\"Processing Batch {batch}\")\n","\n","        # Generate random noise\n","        noise = tf.random.normal([batch_size, latent_dim])\n","\n","        # Prepare labels for generator input\n","        labels_for_generator = tf.reshape(labels, [batch_size, num_classes])\n","        combined_input = tf.concat([noise, labels_for_generator], axis=1)\n","\n","        # Generate fake images\n","        generated_images = generator(combined_input, training=True)\n","\n","        # Expand and tile labels to match image dimensions\n","        labels_expanded = tf.reshape(labels, [batch_size, 1, 1, num_classes])\n","        labels_tiled = tf.tile(labels_expanded, [1, 128, 128, 1])\n","\n","        # Concatenate labels with real and generated images\n","        real_images_with_labels = tf.concat([real_images, labels_tiled], axis=-1)\n","        generated_images_with_labels = tf.concat([generated_images, labels_tiled], axis=-1)\n","\n","        # Train Discriminator\n","        with tf.GradientTape() as disc_tape:\n","            real_output = discriminator(real_images_with_labels, training=True)\n","            fake_output = discriminator(generated_images_with_labels, training=True)\n","\n","            d_loss_real = loss_fn(tf.ones_like(real_output), real_output)\n","            d_loss_fake = loss_fn(tf.zeros_like(fake_output), fake_output)\n","            d_loss = d_loss_real + d_loss_fake\n","\n","        gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n","        d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","        # Train Generator\n","        with tf.GradientTape() as gen_tape:\n","            generated_images_with_labels = tf.concat([generated_images, labels_tiled], axis=-1)\n","            fake_output = discriminator(generated_images_with_labels, training=True)\n","            g_loss = loss_fn(tf.ones_like(fake_output), fake_output)\n","\n","        gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n","        for grad, var in zip(gradients_of_generator, generator.trainable_variables):\n","            if grad is None:\n","                print(f\"None gradient in generator for variable: {var.name}\")\n","        print(\"Gradients of Generator:\", gradients_of_generator)  # Debugging statement\n","\n","        # Check if any gradient is None\n","        if any(grad is None for grad in gradients_of_generator):\n","            raise ValueError(\"Encountered None gradient. Check model architecture and loss function.\")\n","\n","        g_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","\n","        # Print losses to monitor the training process\n","        if batch % 10 == 0:  # Adjust the interval as needed\n","            print(f\"Epoch {epoch}, Batch {batch}, Gen Loss: {g_loss.numpy()}, Disc Loss: {d_loss.numpy()}\")\n","\n","    # Save models at regular intervals (optional)\n","    if epoch % 1 == 0:  # Adjust the interval as needed\n","        generator.save(f'generator_epoch_{epoch}.h5')\n","        discriminator.save(f'discriminator_epoch_{epoch}.h5')\n","        print(f\"End of Epoch {epoch}, Generator Loss: {g_loss.numpy()}, Discriminator Loss: {d_loss.numpy()}\")\n","\n","end_time = time.time()\n","total_time = (end_time - start_time)/60\n","print(f\"Training time: {total_time} mins\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"Vk85tgExgX9Z","executionInfo":{"status":"error","timestamp":1702012523012,"user_tz":360,"elapsed":10876,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"}},"outputId":"572aedfa-be58-423a-cb09-ebe9aee0dc1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start  1702012511.7888947\n","Starting Epoch 0\n","Processing Batch 0\n","None gradient in generator for variable: dense_24/kernel:0\n","None gradient in generator for variable: batch_normalization_48/gamma:0\n","None gradient in generator for variable: batch_normalization_48/beta:0\n","None gradient in generator for variable: conv2d_transpose_48/kernel:0\n","None gradient in generator for variable: batch_normalization_49/gamma:0\n","None gradient in generator for variable: batch_normalization_49/beta:0\n","None gradient in generator for variable: conv2d_transpose_49/kernel:0\n","None gradient in generator for variable: batch_normalization_50/gamma:0\n","None gradient in generator for variable: batch_normalization_50/beta:0\n","None gradient in generator for variable: conv2d_transpose_50/kernel:0\n","None gradient in generator for variable: batch_normalization_51/gamma:0\n","None gradient in generator for variable: batch_normalization_51/beta:0\n","None gradient in generator for variable: conv2d_transpose_51/kernel:0\n","Gradients of Generator: [None, None, None, None, None, None, None, None, None, None, None, None, None]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-ad44f5800b35>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Check if any gradient is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encountered None gradient. Check model architecture and loss function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Encountered None gradient. Check model architecture and loss function."]}]},{"cell_type":"code","source":[],"metadata":{"id":"zxE-FexzgYGR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h2z4aBpHgYI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wNJYlabpgYLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QQYhWRVsgYOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z1XD6pJ7gYRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3IwHVwqxgYUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fToT4l_rjqFD"},"outputs":[],"source":["# # Define real_images and real_labels\n","# real_images, labels = train_ds\n","# real_images_with_labels = concatenate_labels(real_images, labels)\n","# fake_images_with_labels = concatenate_labels(generated_images, labels)\n","\n","# # Then use these concatenated images as input discriminator\n","# real_output = discriminator(real_images_with_labels, training=True)\n","# fake_output = discriminator(fake_images_with_labels, training=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTZBSu7njpzf"},"outputs":[],"source":["# Training hyerparameters\n","EPOCHS = 50\n","noise_dim = 100\n","num_examples_to_generate = 16\n","latent_dim = 100\n","# You will reuse this seed overtime (so it's easier)\n","# to visualize progress in the animated GIF)\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1T53GYS-jqLZ"},"outputs":[],"source":["# # Visualize one of the generated images (for example, the first image in the batch)\n","# plt.imshow(generated_images[0, :, :, 0], cmap='gray')\n","# plt.axis('off')\n","# plt.show()"]},{"cell_type":"code","source":["class cGAN(tf.keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim, batch_size):\n","        super(cGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.batch_size = batch_size\n","        self.num_classes = 4  # Adjusted for 4-class conditional GAN\n","\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(cGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_metric, self.g_loss_metric]\n","\n","    def generator_loss(self, fake_output):\n","        return self.loss_fn(tf.ones_like(fake_output), fake_output)\n","\n","    def discriminator_loss(self, real_output, fake_output):\n","        real_loss = self.loss_fn(tf.ones_like(real_output), real_output)\n","        fake_loss = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n","        total_loss = real_loss + fake_loss\n","        return total_loss\n","    def train_step(self, data):\n","        real_images, labels = data\n","\n","        # Reshape and expand labels to match image dimensions\n","        labels_reshaped = tf.reshape(labels, [self.batch_size, 1, 1, self.num_classes])\n","        labels_expanded = tf.tile(labels_reshaped, [1, 128, 128, 1])\n","\n","        # Generate random noise\n","        noise = tf.random.normal([self.batch_size, self.latent_dim])\n","        combined_input = tf.concat([noise, tf.reshape(labels, [self.batch_size, self.num_classes])], axis=1)\n","\n","        # Generate fake images\n","        generated_images = self.generator(combined_input, training=True)\n","\n","        # Concatenate labels with real and fake images\n","        real_images_with_labels = tf.concat([real_images, labels_expanded], axis=-1)\n","        fake_images_with_labels = tf.concat([generated_images, labels_expanded], axis=-1)\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            # Discriminator predictions\n","            real_output = self.discriminator(real_images_with_labels, training=True)\n","            fake_output = self.discriminator(fake_images_with_labels, training=True)\n","\n","            # Loss calculations\n","            gen_loss = self.generator_loss(fake_output)\n","            disc_loss = self.discriminator_loss(real_output, fake_output)\n","\n","        # Gradient calculations with gradient checking\n","        gen_loss += 1e-8  # Gradient checking\n","        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n","        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n","\n","        if any(g is None for g in gradients_of_generator):\n","            print(\"One or more gradients are None for the generator.\")\n","\n","        # Apply gradients\n","        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n","        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n","\n","        # Update metrics\n","        self.d_loss_metric.update_state(disc_loss)\n","        self.g_loss_metric.update_state(gen_loss)\n","\n","        return {\n","            \"d_loss\": self.d_loss_metric.result(),\n","            \"g_loss\": self.g_loss_metric.result(),\n","        }\n"],"metadata":{"id":"hd2syYs1aQu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjBM8ABukevE"},"outputs":[],"source":["class cGANMonitor(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, num_img=16, latent_dim=latent_dim, num_eval_img=1000):\n","        super().__init__()\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","        self.real_vs_fake_accuracy = []\n","        self.num_eval_img = num_eval_img  # Number of images to evaluate for real vs. fake classification\n","        self.d_losses = []\n","        self.g_losses = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.d_losses.append(logs['d_loss'])\n","        self.g_losses.append(logs['g_loss'])\n","\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        predictions = self.model.generator(random_latent_vectors, training=False)\n","\n","        fig = plt.figure(figsize=(4, 4))\n","        for i in range(predictions.shape[0]):\n","            plt.subplot(4, 4, i+1)\n","            plt.imshow((predictions[i] + 1) / 2)  # Assumes 'tanh' activation in the last layer\n","            plt.axis('off')\n","        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","        plt.close(fig)  # Close the figure after saving to free up memory\n","\n","\n","\n","        # Generate a batch of fake images\n","        random_latent_vectors = tf.random.normal(shape=(self.num_eval_img, self.latent_dim))\n","        fake_images = self.model.generator(random_latent_vectors, training=False)\n","\n","        # Get a batch of real images and add a batch dimension if necessary\n","        real_images = next(iter(train_dataset.take(1)))[0]\n","        if len(real_images.shape) == 3:\n","            real_images = tf.expand_dims(real_images, 0)  # Add batch dimension if it's not there\n","\n","        # Ensure the real_images has the batch size you expect (e.g., self.num_eval_img)\n","        real_images = real_images[:self.num_eval_img]\n","\n","        # Classify real and fake images\n","        real_predictions = self.model.discriminator(real_images, training=False)\n","        fake_predictions = self.model.discriminator(fake_images, training=False)\n","\n","        # Calculate the accuracy\n","        real_accuracy = np.mean(real_predictions > 0.5)\n","        fake_accuracy = np.mean(fake_predictions <= 0.5)\n","        total_accuracy = (real_accuracy + fake_accuracy) / 2\n","\n","        # Log the accuracy\n","        self.real_vs_fake_accuracy.append(total_accuracy)\n"]},{"cell_type":"code","source":["# print(\"Generator Summary:\")\n","# generator.summary()\n","\n","# print(\"Discriminator Summary:\")\n","# discriminator.summary()"],"metadata":{"id":"gFqqUbmoDS-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Example data (fake_output and real_output)\n","# # Replace with actual discriminator outputs for a batch of data\n","# fake_output = discriminator(fake_images_with_labels, training=True)\n","# real_output = discriminator(real_images_with_labels, training=True)\n","# # Calculate generator loss\n","# gen_loss = generator_loss(fake_output)\n","# print(\"Generator Loss:\", gen_loss.numpy())\n","\n","# # Calculate discriminator loss\n","# disc_loss = discriminator_loss(real_output, fake_output)\n","# print(\"Discriminator Loss:\", disc_loss.numpy())\n"],"metadata":{"id":"vz-huXooIQvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Suy-Mq67ker6","executionInfo":{"status":"error","timestamp":1702006425608,"user_tz":360,"elapsed":687,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"}},"outputId":"ddad2e8d-a425-418b-d782-b0a6132ebfa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["After reshape: (None, 8, 8, 1024)\n","After first upsample: (None, 16, 16, 512)\n","After second upsample: (None, 32, 32, 256)\n","After third upsample: (None, 64, 64, 128)\n","After fourth upsample: (None, 128, 128, 1)\n","Epoch 1/50\n","One or more gradients are None for the generator.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-766d7eb7538b>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# After training GAN model with the GANMonitor callback:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcGANMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m gan.fit(train_ds,\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         callbacks=[monitor])\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-c6878ec8696e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_discriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-25-c6878ec8696e>\", line 67, in train_step\n        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1222, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1184, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['dense_4/kernel:0', 'batch_normalization_8/gamma:0', 'batch_normalization_8/beta:0', 'conv2d_transpose_8/kernel:0', 'batch_normalization_9/gamma:0', 'batch_normalization_9/beta:0', 'conv2d_transpose_9/kernel:0', 'batch_normalization_10/gamma:0', 'batch_normalization_10/beta:0', 'conv2d_transpose_10/kernel:0', 'batch_normalization_11/gamma:0', 'batch_normalization_11/beta:0', 'conv2d_transpose_11/kernel:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_4/kernel:0' shape=(104, 65536) dtype=float32>), (None, <tf.Variable 'batch_normalization_8/gamma:0' shape=(65536,) dtype=float32>), (None, <tf.Variable 'batch_normalization_8/beta:0' shape=(65536,) dtype=float32>), (None, <tf.Variable 'conv2d_transpose_8/kernel:0' shape=(5, 5, 512, 1024) dtype=float32>), (None, <tf.Variable 'batch_normalization_9/gamma:0' shape=(512,) dtype=float32>), (None, <tf.Variable 'batch_normalization_9/beta:0' shape=(512,) dtype=float32>), (None, <tf.Variable 'conv2d_transpose_9/kernel:0' shape=(5, 5, 256, 512) dtype=float32>), (None, <tf.Variable 'batch_normalization_10/gamma:0' shape=(256,) dtype=float32>), (None, <tf.Variable 'batch_normalization_10/beta:0' shape=(256,) dtype=float32>), (None, <tf.Variable 'conv2d_transpose_10/kernel:0' shape=(5, 5, 128, 256) dtype=float32>), (None, <tf.Variable 'batch_normalization_11/gamma:0' shape=(128,) dtype=float32>), (None, <tf.Variable 'batch_normalization_11/beta:0' shape=(128,) dtype=...\n"]}],"source":["# Define and compile the generator and discriminator models\n","generator = make_generator_model(num_classes)\n","discriminator = make_discriminator_model(num_classes)\n","\n","# Check if the models have trainable variables\n","if not generator.trainable_variables:\n","    raise ValueError(\"Generator has no trainable variables. Make sure it's properly built and compiled.\")\n","\n","if not discriminator.trainable_variables:\n","    raise ValueError(\"Discriminator has no trainable variables. Make sure it's properly built and compiled.\")\n","\n","\n","#creates an instance of GAN model\n","gan = cGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim, batch_size=batch_size)\n","gan.compile(\n","    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",")\n","\n","# Measure training time\n","start_time = time.time()\n","\n","# After training GAN model with the GANMonitor callback:\n","monitor = cGANMonitor(num_img=16, latent_dim=latent_dim)\n","gan.fit(train_ds,\n","        epochs=EPOCHS,\n","        callbacks=[monitor])\n","\n","# Plot the real vs. fake classification accuracy\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, len(monitor.real_vs_fake_accuracy) + 1), monitor.real_vs_fake_accuracy, label='Real vs. Fake Accuracy', color='purple')\n","plt.title('Discriminator Real vs. Fake Classification Accuracy Over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","end_time = time.time()\n","total_time = (end_time - start_time)/60\n","print(f\"Training time: {total_time} mins\")\n","\n"]},{"cell_type":"code","source":["\n"],"metadata":{"id":"OCLYMctlRvxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZu5Ms8tkepO"},"outputs":[],"source":["gan.save('gan_model')  # 'gan_model' save the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3frsFlwxkegx"},"outputs":[],"source":["for images, labels in train_ds.take(1):\n","    print(\"Images batch shape:\", images.shape)\n","    print(\"Labels batch shape:\", labels.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gHEPItqjpqC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_Z3JVOJjnxJ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}