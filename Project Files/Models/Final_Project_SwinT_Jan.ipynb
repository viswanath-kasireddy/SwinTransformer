{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16307,"status":"ok","timestamp":1702075604830,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"cHrDFUvtU2E1","outputId":"fe56ab0c-91ba-40c1-f9f8-05c34cad6ff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11280,"status":"ok","timestamp":1702075616077,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"wtthz8z0Vwia","outputId":"89074ea3-55df-436b-cda6-dba68da17187"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/ChristophReich1996/Swin-Transformer-V2\n","  Cloning https://github.com/ChristophReich1996/Swin-Transformer-V2 to /tmp/pip-req-build-ppbg_6k6\n","  Running command git clone --filter=blob:none --quiet https://github.com/ChristophReich1996/Swin-Transformer-V2 /tmp/pip-req-build-ppbg_6k6\n","  Resolved https://github.com/ChristophReich1996/Swin-Transformer-V2 to commit 7b74308bd560a5efabc0f9610d038dcf8e61226e\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from swin-transformer-v2==0.1) (2.1.0+cu118)\n","Collecting timm>=0.4.12 (from swin-transformer-v2==0.1)\n","  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm>=0.4.12->swin-transformer-v2==0.1) (0.16.0+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm>=0.4.12->swin-transformer-v2==0.1) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.4.12->swin-transformer-v2==0.1) (0.19.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.4.12->swin-transformer-v2==0.1) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->swin-transformer-v2==0.1) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (4.66.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (23.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->swin-transformer-v2==0.1) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->swin-transformer-v2==0.1) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm>=0.4.12->swin-transformer-v2==0.1) (1.23.5)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm>=0.4.12->swin-transformer-v2==0.1) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm>=0.4.12->swin-transformer-v2==0.1) (2023.11.17)\n","Building wheels for collected packages: swin-transformer-v2\n","  Building wheel for swin-transformer-v2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for swin-transformer-v2: filename=swin_transformer_v2-0.1-py3-none-any.whl size=11083 sha256=9f63074ccd79eb902082f5f30088d9b5e86ae98d81b54362017a78d844bc54bd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3egw5buq/wheels/32/75/e2/de10419d23bc7e671069a9513d00fe74fc8c15ef4972b62f7f\n","Successfully built swin-transformer-v2\n","Installing collected packages: timm, swin-transformer-v2\n","Successfully installed swin-transformer-v2-0.1 timm-0.9.12\n"]}],"source":["pip install git+https://github.com/ChristophReich1996/Swin-Transformer-V2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8161,"status":"ok","timestamp":1702076220287,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"zY9LioRFbC1r","outputId":"78463db3-cfdb-480b-c33c-77a2cfba400b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/44.6 kB\u001b[0m \u001b[31m677.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m757.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}],"source":["pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167154,"status":"ok","timestamp":1702076387435,"user":{"displayName":"Somwipa Lotongkum","userId":"02881437928763858064"},"user_tz":360},"id":"YxV4QDYYhHXf","outputId":"068fffea-b652-40df-ef9c-0d539e25cdb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: tensorflow 2.14.0\n","Uninstalling tensorflow-2.14.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.14.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.14.0\n"]}],"source":["pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9L6XB3Ig3aqt","outputId":"efd26842-418f-4a9e-d75a-8d60ed8c2637"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Found existing installation: keras 2.14.0\n","Uninstalling keras-2.14.0:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/keras-2.14.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/keras/*\n","Proceed (Y/n)? "]}],"source":["pip uninstall keras\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LYYzo8a6jnk"},"outputs":[],"source":["pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09EGKTdx6me8"},"outputs":[],"source":["pip install keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvSTWkzRXkSZ"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from einops import rearrange\n","\n","# Initialize rng\n","rng = np.random.default_rng(2022)\n","\n","\n","import matplotlib.pyplot as plt\n","from keras import layers\n","from keras import ops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bptnHBg1VyzL"},"outputs":[],"source":["batch_size = 32 # This is a tunable hyperparameter\n","shape = (128, 128) # note we are reducing the size of the image\n","# Note: you will use 'grayscale' images for your own model\n","# but you might need to switch to 'rgb' for pretrained models because they are trained on ImageNet which has only RGB images\n","\n","data_dir = '/content/drive/MyDrive/Eye_Dataset/'\n","train_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'train/train'),\n","                                                       seed=rng.integers(500000),\n","                                                       image_size=shape,\n","                                                       label_mode=\"categorical\",\n","                                                       color_mode='grayscale',\n","                                                       batch_size=batch_size)\n","train_ds_new = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'train_new'),\n","                                                       seed=rng.integers(500000),\n","                                                       image_size=shape,\n","                                                       label_mode=\"categorical\",\n","                                                       color_mode='grayscale',\n","                                                       batch_size=batch_size)\n","val_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'validation/validation'),\n","                                                     seed=rng.integers(500000),\n","                                                     image_size=shape,\n","                                                     label_mode=\"categorical\",\n","                                                     color_mode='grayscale',\n","                                                     batch_size=batch_size)\n","test_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir, 'test_new'),\n","                                                      seed=rng.integers(500000),\n","                                                      image_size=shape,\n","                                                      label_mode=\"categorical\",\n","                                                      color_mode='grayscale',\n","                                                      batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNcFmv91z1xd"},"outputs":[],"source":["for x_train,y_train in train_ds_new.take(1):\n","  print(x_train.shape)\n","  print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evWQUve60T2b"},"outputs":[],"source":["for x_val,y_val in val_ds.take(1):\n","  print(x_val.shape)\n","  print(y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbnKCB2g0cul"},"outputs":[],"source":["for x_test,y_test in test_ds.take(1):\n","  print(x_test.shape)\n","  print(y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GywmHCb9Z3Vs"},"outputs":[],"source":["num_classes = 4\n","input_shape = (32, 32, 3)\n","\n","patch_size = (2, 2)  # 2-by-2 sized patches\n","dropout_rate = 0.03  # Dropout rate\n","num_heads = 8  # Attention heads\n","embed_dim = 64  # Embedding dimension\n","num_mlp = 256  # MLP layer size\n","# Convert embedded patches to query, key, and values with a learnable additive\n","# value\n","qkv_bias = True\n","window_size = 2  # Size of attention window\n","shift_size = 1  # Size of shifting window\n","image_dimension = 32  # Initial image size\n","\n","num_patch_x = input_shape[0] // patch_size[0]\n","num_patch_y = input_shape[1] // patch_size[1]\n","\n","learning_rate = 1e-3\n","batch_size = 128\n","num_epochs = 40\n","validation_split = 0.1\n","weight_decay = 0.0001\n","label_smoothing = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VV3NJw2fO6v"},"outputs":[],"source":["# x_train, x_test = x_train / 255.0, x_test / 255.0\n","# y_train = keras.utils.to_categorical(y_train, num_classes)\n","# y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","# num_train_samples = int(len(x_train) * (1 - validation_split))\n","# num_val_samples = len(x_train) - num_train_samples\n","\n","# x_train, x_val = np.split(x_train, [num_train_samples])\n","# y_train, y_val = np.split(y_train, [num_train_samples])\n","\n","# print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","# print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n","\n","# plt.figure(figsize=(10, 10))\n","# for i in range(25):\n","#     plt.subplot(5, 5, i + 1)\n","#     plt.xticks([])\n","#     plt.yticks([])\n","#     plt.grid(False)\n","#     plt.imshow(x_train[i])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Z9N0d4TfUHh"},"outputs":[],"source":["def window_partition(x, window_size):\n","    _, height, width, channels = x.shape\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = ops.reshape(\n","        x,\n","        (\n","            -1,\n","            patch_num_y,\n","            window_size,\n","            patch_num_x,\n","            window_size,\n","            channels,\n","        ),\n","    )\n","    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n","    windows = ops.reshape(x, (-1, window_size, window_size, channels))\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, height, width, channels):\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = ops.reshape(\n","        windows,\n","        (\n","            -1,\n","            patch_num_y,\n","            patch_num_x,\n","            window_size,\n","            window_size,\n","            channels,\n","        ),\n","    )\n","    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n","    x = ops.reshape(x, (-1, height, width, channels))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCrv9pnegGEH"},"outputs":[],"source":["class WindowAttention(layers.Layer):\n","    def __init__(\n","        self,\n","        dim,\n","        window_size,\n","        num_heads,\n","        qkv_bias=True,\n","        dropout_rate=0.0,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        self.scale = (dim // num_heads) ** -0.5\n","        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n","        self.dropout = layers.Dropout(dropout_rate)\n","        self.proj = layers.Dense(dim)\n","\n","        num_window_elements = (2 * self.window_size[0] - 1) * (\n","            2 * self.window_size[1] - 1\n","        )\n","        self.relative_position_bias_table = self.add_weight(\n","            shape=(num_window_elements, self.num_heads),\n","            initializer=keras.initializers.Zeros(),\n","            trainable=True,\n","        )\n","        coords_h = np.arange(self.window_size[0])\n","        coords_w = np.arange(self.window_size[1])\n","        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n","        coords = np.stack(coords_matrix)\n","        coords_flatten = coords.reshape(2, -1)\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","        relative_coords = relative_coords.transpose([1, 2, 0])\n","        relative_coords[:, :, 0] += self.window_size[0] - 1\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)\n","\n","        self.relative_position_index = keras.Variable(\n","            initializer=relative_position_index,\n","            shape=relative_position_index.shape,\n","            dtype=\"int\",\n","            trainable=False,\n","        )\n","\n","    def call(self, x, mask=None):\n","        _, size, channels = x.shape\n","        head_dim = channels // self.num_heads\n","        x_qkv = self.qkv(x)\n","        x_qkv = ops.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n","        x_qkv = ops.transpose(x_qkv, (2, 0, 3, 1, 4))\n","        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n","        q = q * self.scale\n","        k = ops.transpose(k, (0, 1, 3, 2))\n","        attn = q @ k\n","\n","        num_window_elements = self.window_size[0] * self.window_size[1]\n","        relative_position_index_flat = ops.reshape(self.relative_position_index, (-1,))\n","        relative_position_bias = ops.take(\n","            self.relative_position_bias_table,\n","            relative_position_index_flat,\n","            axis=0,\n","        )\n","        relative_position_bias = ops.reshape(\n","            relative_position_bias,\n","            (num_window_elements, num_window_elements, -1),\n","        )\n","        relative_position_bias = ops.transpose(relative_position_bias, (2, 0, 1))\n","        attn = attn + ops.expand_dims(relative_position_bias, axis=0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            mask_float = ops.cast(\n","                ops.expand_dims(ops.expand_dims(mask, axis=1), axis=0),\n","                \"float32\",\n","            )\n","            attn = ops.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n","            attn = ops.reshape(attn, (-1, self.num_heads, size, size))\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        else:\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        attn = self.dropout(attn)\n","\n","        x_qkv = attn @ v\n","        x_qkv = ops.transpose(x_qkv, (0, 2, 1, 3))\n","        x_qkv = ops.reshape(x_qkv, (-1, size, channels))\n","        x_qkv = self.proj(x_qkv)\n","        x_qkv = self.dropout(x_qkv)\n","        return x_qkv\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTeYQkPFgfzM"},"outputs":[],"source":["class SwinTransformer(layers.Layer):\n","    def __init__(\n","        self,\n","        dim,\n","        num_patch,\n","        num_heads,\n","        window_size=7,\n","        shift_size=0,\n","        num_mlp=1024,\n","        qkv_bias=True,\n","        dropout_rate=0.0,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.dim = dim  # number of input dimensions\n","        self.num_patch = num_patch  # number of embedded patches\n","        self.num_heads = num_heads  # number of attention heads\n","        self.window_size = window_size  # size of window\n","        self.shift_size = shift_size  # size of window shift\n","        self.num_mlp = num_mlp  # number of MLP nodes\n","\n","        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=(self.window_size, self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            dropout_rate=dropout_rate,\n","        )\n","        self.drop_path = layers.Dropout(dropout_rate)\n","        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n","\n","        self.mlp = keras.Sequential(\n","            [\n","                layers.Dense(num_mlp),\n","                layers.Activation(keras.activations.gelu),\n","                layers.Dropout(dropout_rate),\n","                layers.Dense(dim),\n","                layers.Dropout(dropout_rate),\n","            ]\n","        )\n","\n","        if min(self.num_patch) < self.window_size:\n","            self.shift_size = 0\n","            self.window_size = min(self.num_patch)\n","\n","    def build(self, input_shape):\n","        if self.shift_size == 0:\n","            self.attn_mask = None\n","        else:\n","            height, width = self.num_patch\n","            h_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            w_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            mask_array = np.zeros((1, height, width, 1))\n","            count = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    mask_array[:, h, w, :] = count\n","                    count += 1\n","            mask_array = ops.convert_to_tensor(mask_array)\n","\n","            # mask array to windows\n","            mask_windows = window_partition(mask_array, self.window_size)\n","            mask_windows = ops.reshape(\n","                mask_windows, [-1, self.window_size * self.window_size]\n","            )\n","            attn_mask = ops.expand_dims(mask_windows, axis=1) - ops.expand_dims(\n","                mask_windows, axis=2\n","            )\n","            attn_mask = ops.where(attn_mask != 0, -100.0, attn_mask)\n","            attn_mask = ops.where(attn_mask == 0, 0.0, attn_mask)\n","            self.attn_mask = keras.Variable(\n","                initializer=attn_mask,\n","                shape=attn_mask.shape,\n","                dtype=attn_mask.dtype,\n","                trainable=False,\n","            )\n","\n","    def call(self, x, training=False):\n","        height, width = self.num_patch\n","        _, num_patches_before, channels = x.shape\n","        x_skip = x\n","        x = self.norm1(x)\n","        x = ops.reshape(x, (-1, height, width, channels))\n","        if self.shift_size > 0:\n","            shifted_x = ops.roll(\n","                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            shifted_x = x\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = ops.reshape(\n","            x_windows, (-1, self.window_size * self.window_size, channels)\n","        )\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n","\n","        attn_windows = ops.reshape(\n","            attn_windows,\n","            (-1, self.window_size, self.window_size, channels),\n","        )\n","        shifted_x = window_reverse(\n","            attn_windows, self.window_size, height, width, channels\n","        )\n","        if self.shift_size > 0:\n","            x = ops.roll(\n","                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            x = shifted_x\n","\n","        x = ops.reshape(x, (-1, height * width, channels))\n","        x = self.drop_path(x, training=training)\n","        x = x_skip + x\n","        x_skip = x\n","        x = self.norm2(x)\n","        x = self.mlp(x)\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhuDZXmXgg9b"},"outputs":[],"source":["# Using tf ops since it is only used in tf.data.\n","def patch_extract(images):\n","    batch_size = tf.shape(images)[0]\n","    patches = tf.image.extract_patches(\n","        images=images,\n","        sizes=(1, patch_size[0], patch_size[1], 1),\n","        strides=(1, patch_size[0], patch_size[1], 1),\n","        rates=(1, 1, 1, 1),\n","        padding=\"VALID\",\n","    )\n","    patch_dim = patches.shape[-1]\n","    patch_num = patches.shape[1]\n","    return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n","\n","\n","class PatchEmbedding(layers.Layer):\n","    def __init__(self, num_patch, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.num_patch = num_patch\n","        self.proj = layers.Dense(embed_dim)\n","        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n","\n","    def call(self, patch):\n","        pos = ops.arange(start=0, stop=self.num_patch)\n","        return self.proj(patch) + self.pos_embed(pos)\n","\n","\n","class PatchMerging(keras.layers.Layer):\n","    def __init__(self, num_patch, embed_dim):\n","        super().__init__()\n","        self.num_patch = num_patch\n","        self.embed_dim = embed_dim\n","        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, _, C = x.shape\n","        x = ops.reshape(x, (-1, height, width, C))\n","        x0 = x[:, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, :]\n","        x3 = x[:, 1::2, 1::2, :]\n","        x = ops.concatenate((x0, x1, x2, x3), axis=-1)\n","        x = ops.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n","        return self.linear_trans(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eivvroZ_1OWF"},"outputs":[],"source":["def augment(x):\n","    x = tf.image.random_crop(x, size=(image_dimension, image_dimension, 3))\n","    x = tf.image.random_flip_left_right(x)\n","    return x\n","\n","\n","dataset = (\n","    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","    .map(lambda x, y: (augment(x), y))\n","    .batch(batch_size=batch_size)\n","    .map(lambda x, y: (patch_extract(x), y))\n","    .prefetch(tf.data.experimental.AUTOTUNE)\n",")\n","\n","dataset_val = (\n","    tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","    .batch(batch_size=batch_size)\n","    .map(lambda x, y: (patch_extract(x), y))\n","    .prefetch(tf.data.experimental.AUTOTUNE)\n",")\n","\n","dataset_test = (\n","    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","    .batch(batch_size=batch_size)\n","    .map(lambda x, y: (patch_extract(x), y))\n","    .prefetch(tf.data.experimental.AUTOTUNE)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":596},"executionInfo":{"elapsed":329,"status":"error","timestamp":1701978741769,"user":{"displayName":"Jay Mapanao","userId":"12282629067149059521"},"user_tz":360},"id":"Y9BB27es1USC","outputId":"49705783-1fc9-4ad7-fad4-13bad56ba164"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-7d81a7aaba17>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_patch_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_patch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m x = SwinTransformer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_patch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_patch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_filey9ypdf_6.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: Exception encountered when calling layer \"patch_embedding\" (type PatchEmbedding).\n\nin user code:\n\n    File \"<ipython-input-51-d42a590623f6>\", line 24, in call  *\n        pos = ops.arange(start=0, stop=self.num_patch)\n\n    NameError: name 'ops' is not defined\n\n\nCall arguments received by layer \"patch_embedding\" (type PatchEmbedding):\n  • patch=tf.Tensor(shape=(None, 256, 12), dtype=float32)"]}],"source":["input = layers.Input(shape=(256, 12))\n","x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(input)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=0,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=shift_size,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","output = layers.Dense(num_classes, activation=\"softmax\")(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3F_KDl41Yww"},"outputs":[],"source":["model = keras.Model(input, output)\n","model.compile(\n","    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n","    optimizer=keras.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    ),\n","    metrics=[\n","        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n","        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","    ],\n",")\n","\n","history = model.fit(\n","    dataset,\n","    batch_size=batch_size,\n","    epochs=num_epochs,\n","    validation_data=dataset_val,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDxwnDZz1ZT4"},"outputs":[],"source":["plt.plot(history.history[\"loss\"], label=\"train_loss\")\n","plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgL-_fyJ1eEn"},"outputs":[],"source":["loss, accuracy, top_5_accuracy = model.evaluate(dataset_test)\n","print(f\"Test loss: {round(loss, 2)}\")\n","print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}